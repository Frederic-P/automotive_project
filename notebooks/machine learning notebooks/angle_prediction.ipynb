{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car angle predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import sys\n",
    "sys.path.append('../../utils')\n",
    "import config_handling as conf\n",
    "from database import Database\n",
    "from file_io import path_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n"
     ]
    }
   ],
   "source": [
    "config = conf.read_config('../../config/automotive.conf.ini')\n",
    "config.read('config.ini')\n",
    "connection_type = config['settings']['connection']\n",
    "connection_type\n",
    "user = config[connection_type]['user']\n",
    "pw = config[connection_type]['pw']\n",
    "host = config[connection_type]['host']\n",
    "db = config[connection_type]['db']\n",
    "port = config[connection_type].getint('port')\n",
    "db = Database(host,\n",
    "              port,\n",
    "              user,\n",
    "              pw,\n",
    "              db\n",
    "              )\n",
    "db.connect()\n",
    "#image directory: \n",
    "basedir = config['settings']['image_directory']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tags_query = \"\"\"\n",
    "    SELECT * FROM angletags \n",
    "    JOIN images ON images.id = angletags.image_id\n",
    "    WHERE angletags.manual_annotation = 1\n",
    "\"\"\"\n",
    "r = db.execute_query(get_tags_query, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(r)\n",
    "data.sample(5)\n",
    "del(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>angle</th>\n",
       "      <th>manual_annotation</th>\n",
       "      <th>pk</th>\n",
       "      <th>id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>processed</th>\n",
       "      <th>use_image</th>\n",
       "      <th>certainty_of_outside_yolo</th>\n",
       "      <th>yolobox_top_left_x</th>\n",
       "      <th>yolobox_top_left_y</th>\n",
       "      <th>yolobox_bottom_right_x</th>\n",
       "      <th>yolobox_bottom_right_y</th>\n",
       "      <th>area</th>\n",
       "      <th>confidence</th>\n",
       "      <th>image_path</th>\n",
       "      <th>abs_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>1305352</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2483</td>\n",
       "      <td>1305352</td>\n",
       "      <td>83867</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>100.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>0.552311</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>kia\\Carens\\868c58f0-fab0-4e78-9ab0-d126d21f9d1...</td>\n",
       "      <td>Z:\\kia\\Carens\\868c58f0-fab0-4e78-9ab0-d126d21f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>828137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>614</td>\n",
       "      <td>828137</td>\n",
       "      <td>52953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mercedes-benz\\EQ-Klasse (alle)\\d2e79130-d705-4...</td>\n",
       "      <td>Z:\\mercedes-benz\\EQ-Klasse (alle)\\d2e79130-d70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>577402</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>577402</td>\n",
       "      <td>36787</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volkswagen\\Beetle\\4e0a8618-2fdb-4129-963f-6e00...</td>\n",
       "      <td>Z:\\volkswagen\\Beetle\\4e0a8618-2fdb-4129-963f-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1061130</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>884</td>\n",
       "      <td>1061130</td>\n",
       "      <td>68496</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>opel\\Astra\\3ed28570-a20d-45c6-a2b8-24f5969b34d...</td>\n",
       "      <td>Z:\\opel\\Astra\\3ed28570-a20d-45c6-a2b8-24f5969b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id  angle  manual_annotation    pk       id  listing_id  \\\n",
       "2481   1305352      6                  1  2483  1305352       83867   \n",
       "613     828137      1                  1   614   828137       52953   \n",
       "342     577402      1                  1   343   577402       36787   \n",
       "883    1061130      1                  1   884  1061130       68496   \n",
       "\n",
       "      processed  use_image certainty_of_outside_yolo  yolobox_top_left_x  \\\n",
       "2481          1          1                      None               100.0   \n",
       "613           1          0                      None                 NaN   \n",
       "342           1          0                      None                 NaN   \n",
       "883           1          0                      None                 NaN   \n",
       "\n",
       "      yolobox_top_left_y  yolobox_bottom_right_x  yolobox_bottom_right_y  \\\n",
       "2481                67.0                   593.0                   498.0   \n",
       "613                  NaN                     NaN                     NaN   \n",
       "342                  NaN                     NaN                     NaN   \n",
       "883                  NaN                     NaN                     NaN   \n",
       "\n",
       "          area  confidence                                         image_path  \\\n",
       "2481  0.552311    0.757042  kia\\Carens\\868c58f0-fab0-4e78-9ab0-d126d21f9d1...   \n",
       "613        NaN         NaN  mercedes-benz\\EQ-Klasse (alle)\\d2e79130-d705-4...   \n",
       "342        NaN         NaN  volkswagen\\Beetle\\4e0a8618-2fdb-4129-963f-6e00...   \n",
       "883        NaN         NaN  opel\\Astra\\3ed28570-a20d-45c6-a2b8-24f5969b34d...   \n",
       "\n",
       "                                               abs_path  \n",
       "2481  Z:\\kia\\Carens\\868c58f0-fab0-4e78-9ab0-d126d21f...  \n",
       "613   Z:\\mercedes-benz\\EQ-Klasse (alle)\\d2e79130-d70...  \n",
       "342   Z:\\volkswagen\\Beetle\\4e0a8618-2fdb-4129-963f-6...  \n",
       "883   Z:\\opel\\Astra\\3ed28570-a20d-45c6-a2b8-24f5969b...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROJECT CONSTANTS CONFIGURATION OPTIONS\n",
    "# Configuration\n",
    "USE_BBOX = False  # Toggle this to use bounding box coordinates\n",
    "IMG_SIZE = (128, 128)  # Image dimensions\n",
    "BATCH_SIZE = 32\n",
    "#from DB\n",
    "CLASSES = data.angle.unique()\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "CLASS_MAPPING = {cls: idx for idx, cls in enumerate(CLASSES)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map angle to integers\n",
    "data['angle'] = data['angle'].map(CLASS_MAPPING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_image(image_path, bbox=None):\n",
    "    image = tf.keras.utils.load_img(image_path, target_size=IMG_SIZE)\n",
    "    image = tf.keras.utils.img_to_array(image) / 255.0\n",
    "    if USE_BBOX and bbox is not None:\n",
    "        #crop image if bbox constant is true.\n",
    "        bbox = np.array(bbox) / np.array([IMG_SIZE[1], IMG_SIZE[0], IMG_SIZE[1], IMG_SIZE[0]])\n",
    "        return image, bbox\n",
    "    return image, None\n",
    "\n",
    "# Data Generator\n",
    "def data_generator(df, batch_size=BATCH_SIZE):\n",
    "    images = []\n",
    "    labels = []\n",
    "    while True:\n",
    "        for _, row in df.iterrows():\n",
    "            image, bbox = preprocess_image(\n",
    "                row['abs_path'],\n",
    "                [row['yolobox_top_left_x'], row['yolobox_top_left_y'], row['yolobox_bottom_right_x'], row['yolobox_bottom_right_y']]\n",
    "            )\n",
    "            label = row['angle']\n",
    "            images.append(image)\n",
    "            labels.append(tf.keras.utils.to_categorical(label, num_classes=NUM_CLASSES))\n",
    "\n",
    "            if len(images) == batch_size:\n",
    "                yield np.array(images), np.array(labels)\n",
    "                images, labels = [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine config readable basedir with the relative path in the dataframe: \n",
    "data['abs_path'] = data.apply(lambda row: path_handler(basedir, row.image_path), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "train_df = data.sample(frac=0.8, random_state=42)\n",
    "val_df = data.drop(train_df.index)\n",
    "\n",
    "# Create Datasets\n",
    "train_gen = data_generator(train_df)\n",
    "val_gen = data_generator(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frede\\Documents\\GitHub\\automotive_project\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "if USE_BBOX:\n",
    "    image_input = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3), name=\"image\")\n",
    "    bbox_input = Input(shape=(4,), name=\"bbox\")\n",
    "    \n",
    "    # Image branch\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Combine image and bbox inputs\n",
    "    combined = Concatenate()([x, bbox_input])\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = Dense(128, activation='relu')(combined)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[image_input, bbox_input], outputs=output)\n",
    "else:\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 279ms/step - accuracy: 0.4407 - loss: 18.1623 - val_accuracy: 0.5918 - val_loss: 1.5702\n",
      "Epoch 2/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 298ms/step - accuracy: 0.5084 - loss: 1.5877 - val_accuracy: 0.6035 - val_loss: 1.3248\n",
      "Epoch 3/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 276ms/step - accuracy: 0.5791 - loss: 1.3006 - val_accuracy: 0.6133 - val_loss: 1.1726\n",
      "Epoch 4/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 274ms/step - accuracy: 0.6169 - loss: 1.1124 - val_accuracy: 0.5957 - val_loss: 1.1120\n",
      "Epoch 5/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 279ms/step - accuracy: 0.6470 - loss: 0.9384 - val_accuracy: 0.6445 - val_loss: 1.0579\n",
      "Epoch 6/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 281ms/step - accuracy: 0.6611 - loss: 0.8577 - val_accuracy: 0.6309 - val_loss: 1.1207\n",
      "Epoch 7/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 281ms/step - accuracy: 0.6789 - loss: 0.7586 - val_accuracy: 0.6289 - val_loss: 1.1242\n",
      "Epoch 8/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 283ms/step - accuracy: 0.7098 - loss: 0.7201 - val_accuracy: 0.6191 - val_loss: 1.2628\n",
      "Epoch 9/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 284ms/step - accuracy: 0.7203 - loss: 0.6980 - val_accuracy: 0.6777 - val_loss: 1.2243\n",
      "Epoch 10/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 282ms/step - accuracy: 0.7425 - loss: 0.6023 - val_accuracy: 0.6914 - val_loss: 1.1630\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch = len(train_df) // BATCH_SIZE,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps = len(val_df) // BATCH_SIZE,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Results:\n",
      "(128, 128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frede\\Documents\\GitHub\\automotive_project\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_6\n",
      "Received: inputs=('Tensor(shape=(32, 128, 3))',)\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 128, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (32, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32, 128, 3), dtype=float32)',)\n  • training=False\n  • mask=('None',)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Set Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(df, generator)\u001b[0m\n\u001b[0;32m     12\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([[image], [bbox]], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     pred_labels\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(prediction))\n\u001b[0;32m     17\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(true_labels, pred_labels)\n",
      "File \u001b[1;32mc:\\Users\\Frede\\Documents\\GitHub\\automotive_project\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Frede\\Documents\\GitHub\\automotive_project\\Lib\\site-packages\\keras\\src\\models\\functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 128, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (32, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32, 128, 3), dtype=float32)',)\n  • training=False\n  • mask=('None',)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation and Confusion Matrix\n",
    "def evaluate_model(df, generator):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        image, bbox = preprocess_image(row['abs_path'], [row['yolobox_top_left_x'], row['yolobox_top_left_y'], row['yolobox_bottom_right_x'], row['yolobox_bottom_right_y']])\n",
    "        label = row['angle']\n",
    "        true_labels.append(label)\n",
    "        print(image.shape)\n",
    "        \n",
    "        if USE_BBOX and bbox is not None:\n",
    "            prediction = model.predict([[image], [bbox]], verbose=0)\n",
    "        else:\n",
    "            prediction = model.predict([image], verbose=0)\n",
    "        pred_labels.append(np.argmax(prediction))\n",
    "\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(true_labels, pred_labels, target_names=CLASSES))\n",
    "\n",
    "    # Custom Proximity Evaluation\n",
    "    proximity_score = 0\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_idx = CLASSES.index(CLASSES[true])\n",
    "        pred_idx = CLASSES.index(CLASSES[pred])\n",
    "        proximity_score += 1 / (1 + abs(true_idx - pred_idx))  # Higher score for closer predictions\n",
    "    proximity_score /= len(true_labels)\n",
    "\n",
    "    print(f\"Custom Proximity Score: {proximity_score:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Set Results:\")\n",
    "evaluate_model(val_df, val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 597ms/step - accuracy: 0.9914 - loss: 0.0259 - val_accuracy: 0.7480 - val_loss: 1.6192\n",
      "Epoch 2/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 590ms/step - accuracy: 0.9927 - loss: 0.0341 - val_accuracy: 0.7363 - val_loss: 1.7864\n",
      "Epoch 3/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 603ms/step - accuracy: 0.9935 - loss: 0.0282 - val_accuracy: 0.7168 - val_loss: 1.8866\n",
      "Epoch 4/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 624ms/step - accuracy: 0.9968 - loss: 0.0163 - val_accuracy: 0.7070 - val_loss: 2.1218\n",
      "Epoch 5/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 650ms/step - accuracy: 0.9979 - loss: 0.0098 - val_accuracy: 0.7012 - val_loss: 2.3396\n",
      "Epoch 6/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 617ms/step - accuracy: 0.9970 - loss: 0.0176 - val_accuracy: 0.7266 - val_loss: 2.1610\n",
      "Epoch 7/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 600ms/step - accuracy: 0.9937 - loss: 0.0285 - val_accuracy: 0.6895 - val_loss: 2.5171\n",
      "Epoch 8/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 602ms/step - accuracy: 0.9939 - loss: 0.0222 - val_accuracy: 0.7207 - val_loss: 1.8450\n",
      "Epoch 9/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 612ms/step - accuracy: 0.9928 - loss: 0.0194 - val_accuracy: 0.7090 - val_loss: 2.1304\n",
      "Epoch 10/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 644ms/step - accuracy: 0.9925 - loss: 0.0372 - val_accuracy: 0.6777 - val_loss: 2.4083\n",
      "Validation Set Results:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 128, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (32, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32, 128, 3), dtype=float32)',)\n  • training=False\n  • mask=('None',)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Set Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(df, generator)\u001b[0m\n\u001b[0;32m     23\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([[image], [bbox]], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     pred_labels\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(prediction))\n\u001b[0;32m     28\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(true_labels, pred_labels)\n",
      "File \u001b[1;32mc:\\Users\\Frede\\Documents\\GitHub\\automotive_project\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Frede\\Documents\\GitHub\\automotive_project\\Lib\\site-packages\\keras\\src\\models\\functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 128, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (32, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32, 128, 3), dtype=float32)',)\n  • training=False\n  • mask=('None',)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_df) // BATCH_SIZE,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=len(val_df) // BATCH_SIZE,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Evaluation and Confusion Matrix\n",
    "def evaluate_model(df, generator):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        image, bbox = preprocess_image(row['abs_path'], [row['yolobox_top_left_x'], row['yolobox_top_left_y'], row['yolobox_bottom_right_x'], row['yolobox_bottom_right_y']])\n",
    "        label = row['angle']\n",
    "        true_labels.append(label)\n",
    "        \n",
    "        if USE_BBOX and bbox is not None:\n",
    "            prediction = model.predict([[image], [bbox]], verbose=0)\n",
    "        else:\n",
    "            prediction = model.predict([image], verbose=0)\n",
    "        pred_labels.append(np.argmax(prediction))\n",
    "\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(true_labels, pred_labels, target_names=CLASSES))\n",
    "\n",
    "    # Custom Proximity Evaluation\n",
    "    proximity_score = 0\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_idx = CLASSES.index(CLASSES[true])\n",
    "        #TODOhereyouneedtomakeacircularscoringthingy;-1iscloseto0alsofixwhymyspacebarisnotworking!\n",
    "        pred_idx = CLASSES.index(CLASSES[pred])\n",
    "        proximity_score += 1 / (1 + abs(true_idx - pred_idx))  # Higher score for closer predictions\n",
    "    proximity_score /= len(true_labels)\n",
    "\n",
    "    print(f\"Custom Proximity Score: {proximity_score:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Set Results:\")\n",
    "evaluate_model(val_df, val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automotive_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
