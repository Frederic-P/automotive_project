{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625eb8c4-9540-41d7-a948-e13b9f008238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "sys.path.append('utils')\n",
    "import os\n",
    "import config_handling as conf\n",
    "from multithread_image_ripper import download_images\n",
    "from database import Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab4a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "config = conf.read_config('automotive.conf.ini')\n",
    "config.read('config.ini')\n",
    "connection_type = config['settings']['connection']\n",
    "connection_type\n",
    "user = config[connection_type]['user']\n",
    "pw = config[connection_type]['pw']\n",
    "host = config[connection_type]['host']\n",
    "db = config[connection_type]['db']\n",
    "port = config[connection_type].getint('port')\n",
    "db = Database(host,\n",
    "              port,\n",
    "              user,\n",
    "              pw,\n",
    "              db\n",
    "              )\n",
    "db.connect()\n",
    "db.start_transaction()\n",
    "#image directory: \n",
    "basedir = config['settings']['image_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e4ab8-7e9a-4970-960d-b18a84c3c8a9",
   "metadata": {},
   "source": [
    "## 1. Brand extraction: \n",
    "Start by getting all the brand codes, these do not match the codes used by Autodoc in the previous step of the data scraping process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7b6cbc-58f7-4ebf-a4f7-d663996ffdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at = \"https://www.autoscout24.be/nl/\"\n",
    "# basedir = 'C:\\imdir'\n",
    "session = requests.Session()\n",
    "r = session.get(start_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90348d56-506e-49b9-9008-81bd66fe43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(given_name): \n",
    "    # Lowers and removes all non-alphanumeric characters from a given input string,\n",
    "    # and replaces spaces with hyphens.\n",
    "    special = [' ', '-', '/', '(', ')', '.']\n",
    "    alphanums = list([symbol for symbol in given_name if symbol.isalpha() or symbol.isnumeric() or symbol in special])\n",
    "    normalized_name = ''.join(alphanums).lower()\n",
    "    normalized_name = normalized_name.replace('/','%2f')\n",
    "    return normalized_name.replace(' ', '-')\n",
    "\n",
    "r.content\n",
    "soup = BeautifulSoup(r.content)\n",
    "brands = soup.select(\"#make\")[0].find_all('option')\n",
    "l = {}\n",
    "for brand in brands:\n",
    "    value = brand['value']\n",
    "    name = normalize_name(brand.text)\n",
    "    if value != '': \n",
    "        l[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1013609-299a-4053-8175-d81300967a4f",
   "metadata": {},
   "source": [
    "## 2. Model extraction\n",
    "For model extraction we can use XHR requests again to a JSON endpoint (for this we need the numerical brand id from the previous extraction step). Model hierarchy is not the same for all brands.\n",
    "\n",
    "For instance the BMW 1-series is a valid model designation, autoscout has this as a parent element for 1-series cars with different engines in it (e.g. 114, 116, 120). In this case xyy can be interpreted as: x = series designation, yy = engine displacement /100 (114 has a 1400CC engine, 116 has a 1600CC engine...). Each of the different engine displacement is considered to be a seperate submodel to the 1 series by Autoscout - even if there's visually no difference between the cars.\n",
    "The problem is with X-series carsfor BMW, where X1, X3, X5 are distinct cars but are all grouped in the same 'family' of cars.\n",
    "\n",
    "For other brands there is no such hierarchy, e.g. Toyota where the response is a flat list of modelnames without differntiating different bodystyles (e.g. Corrola hatch or Corrola SW). Here the data will return both SW and hatch body styles for the same model. The visual difference between an SW and a hatch model is quite stark and easy to recognize for the human observer. \n",
    "\n",
    "In spite of these problems, it would be a good idea to have the scraper collect model by model - in stead of collecting all data for the entire brand. The advantage of this approach is that you have an identifiable target. (i.e. you know the car is of brand X and model Y), then you can use features on autoscout such as doorcount, inscription year and bodystyle (which all are subject to human errors); to correctly link it to chassis codes extracted from autodoc. \n",
    "\n",
    "One caveat with the inscription year is that this value is not the same as a production year!!! Generally speaking production starts well in advance of car sales and older stock models can still be registered while the new model is in production. This means there'll be an overlap - not much to do about it; we might consider dropping the first three months of sales for a given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fd378b-19a6-42f4-8e41-5188aa55be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brand = 'lexus'\n",
    "# print(l[brand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = {\n",
    "#     \"Accept\": \"application/json\", \n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "\n",
    "# }\n",
    "# #NOTE: API requests are rejected if you don't have a session cookie; that's why we use session.get and not the default request.get\n",
    "# url = f\"https://www.autoscout24.be/as24-home/api/taxonomy/cars/makes/{l[brand]}/models\"\n",
    "# r = session.get(url, headers= headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd5a7ee-8652-4a1e-8e55-efe167fd184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #IF the modelLineId is None, than the model_id is a code on itself. (Toyota casus)\n",
    "# # IF modelLineId is not None, then the model_id is to be considered a subcode of the modelLineId (BMW casus)\n",
    "# models = []\n",
    "# model_lines = []\n",
    "# for model in r.json()['models']['model']['values']:\n",
    "#     model_name = model['name']\n",
    "#     model_id = model['id']\n",
    "#     models.append([model_name, model_id, model['modelLineId']])\n",
    "# for model_line in r.json()['models']['modelLine']['values']:\n",
    "#     model_line_id = model_line['id']\n",
    "#     model_line_name = model_line['name']\n",
    "#     dutch_name = model_line['label']['nl_BE']\n",
    "#     model_lines.append([model_line_id, model_line_name, dutch_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27f156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df = pd.DataFrame(models, columns=['modelname','model_id','parent_id'])\n",
    "# model_lines_df = pd.DataFrame(model_lines, columns=['line_id', 'modelline_name', 'labelnaam_requests'])\n",
    "\n",
    "# models_merged = pd.merge(model_df, model_lines_df, left_on='parent_id', right_on='line_id', how='outer')  # You can change 'inner' to 'outer', 'left', or 'right' based on your requirement\n",
    "\n",
    "# models_merged['request_parameter'] = np.where(\n",
    "#     models_merged['labelnaam_requests'].isna(),\n",
    "#     models_merged['modelname'],\n",
    "#     models_merged['labelnaam_requests']\n",
    "# )\n",
    "# models_merged\n",
    "# assert(models_merged.request_parameter.isna().sum() == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486ff7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0629c9",
   "metadata": {},
   "source": [
    "Autoscout allows a max of 20 pages with 20 results each. So you have a hard limit of 400 listings for a given brand/model combination. We don't care much about pagination, as exceeding the first 400 results will automatically show the same message as 'all results have been viewed'. We can solve the issue by restricting registration years X to Y where the range X-Y 0 (iterating over all cars from a specific year)\n",
    "\n",
    "when dealing with images downloaded from Autoscout, you'll have the data as compressed files (.webp), YOLO does not support this and I don't know about other ML models, so the multithreaded ripper should have .jpg conversion built in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0294b996-90d8-46e4-a6d8-302a598ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping listings per brand/model: \n",
    "def backoff_strategy(at, extract_json = False):\n",
    "    success = False\n",
    "    x = 1\n",
    "    while not success and x <= 6: \n",
    "        r = session.get(at)\n",
    "        if r.status_code == 200:\n",
    "            if not extract_json:\n",
    "                success = True\n",
    "                return r\n",
    "            else:\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "                json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "                json_data = json.loads(json_string_data.string)\n",
    "                valid_parse = (\n",
    "                    'props' in json_data and\n",
    "                    'pageProps' in json_data['props'] and\n",
    "                    'listingDetails' in json_data['props']['pageProps'] and\n",
    "                    'vehicle' in json_data['props']['pageProps']['listingDetails']\n",
    "                )\n",
    "                if json_data is not None and valid_parse:\n",
    "                    success = True\n",
    "                    return json_data\n",
    "        time.sleep(x*10)\n",
    "        x+=1\n",
    "    return r\n",
    "\n",
    "def extract_dates(brand, model):\n",
    "    earliest_year_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}?atype=C&cy=B&damaged_listing=exclude&desc=0&sort=year\"\n",
    "    latest_year_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}?atype=C&cy=B&damaged_listing=exclude&desc=1&sort=year\"\n",
    "    # r_start = session.get(earliest_year_url)\n",
    "    r_start = backoff_strategy(earliest_year_url)\n",
    "    soup_start = BeautifulSoup(r_start.content, 'html.parser')\n",
    "    first_articles = soup_start.find_all('article')\n",
    "    if len(first_articles) == 0:    #No cars for sale\n",
    "        return[False, False]\n",
    "    first_date = first_articles[0].get('data-first-registration')\n",
    "    # r_stop = session.get(latest_year_url)   #defintely cars for sale, no need to check.\n",
    "    r_stop = backoff_strategy(latest_year_url)\n",
    "    soup_stop = BeautifulSoup(r_stop.content, 'html.parser')\n",
    "    last_date = soup_stop.find_all('article')[0].get('data-first-registration')\n",
    "    if first_date.lower() == 'new':\n",
    "        first_year = 2024\n",
    "    else:\n",
    "        first_year = first_date.split('-')[1]\n",
    "    if last_date.lower() == 'new' or last_date.lower() == 'unknown':\n",
    "        last_year = 2024\n",
    "    else:\n",
    "        last_year = last_date.split('-')[1]\n",
    "    return [int(first_year), int(last_year)]\n",
    "\n",
    "def get_listing_details(listing):\n",
    "    \"\"\"doorcount and chassistype would be handy to have, it's unfortunately not part of the\n",
    "    JSON response so we need to do one request per listing to extract this usefull information.\n",
    "    There are other useful bits of information too that might help the disambiguation process.\n",
    "\n",
    "    The good thing is it's quite easy to parse as they made it accessible in JSON format\n",
    "    \"\"\"\n",
    "    url = f\"https://www.autoscout24.be/nl/aanbod/{listing}\"\n",
    "    json_data = backoff_strategy(url, True)\n",
    "    # soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    # json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "    # json_data = json.loads(json_string_data.string)\n",
    "    data = json_data['props']['pageProps']['listingDetails']['vehicle']\n",
    "    shell = data['bodyType']\n",
    "    doors = data['numberOfDoors']\n",
    "    weight = data['weight']\n",
    "    norm_data = data['environmentEuDirective']\n",
    "    if norm_data is not None:\n",
    "        norm = norm_data['label']\n",
    "    else:\n",
    "        norm = None\n",
    "    return [shell, doors, weight, norm]\n",
    "\n",
    "def get_models(brand): \n",
    "    headers = {\n",
    "    \"Accept\": \"application/json\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    #NOTE: API requests are rejected if you don't have a session cookie; that's why we use session.get and not the default request.get\n",
    "    url = f\"https://www.autoscout24.be/as24-home/api/taxonomy/cars/makes/{l[brand]}/models\"\n",
    "    r = session.get(url, headers= headers)\n",
    "    #IF the modelLineId is None, than the model_id is a code on itself. (Toyota casus)\n",
    "    # IF modelLineId is not None, then the model_id is to be considered a subcode of the modelLineId (BMW casus)\n",
    "    models = []\n",
    "    model_lines = []\n",
    "    for model in r.json()['models']['model']['values']:\n",
    "        model_name = model['name']\n",
    "        model_id = model['id']\n",
    "        models.append([model_name, model_id, model['modelLineId']])\n",
    "    for model_line in r.json()['models']['modelLine']['values']:\n",
    "        model_line_id = model_line['id']\n",
    "        model_line_name = model_line['name']\n",
    "        dutch_name = model_line['label']['nl_BE']\n",
    "        model_lines.append([model_line_id, model_line_name, dutch_name])\n",
    "    model_df = pd.DataFrame(models, columns=['modelname','model_id','parent_id'])\n",
    "    model_lines_df = pd.DataFrame(model_lines, columns=['line_id', 'modelline_name', 'labelnaam_requests'])\n",
    "\n",
    "    models_merged = pd.merge(model_df, model_lines_df, left_on='parent_id', right_on='line_id', how='outer')  # You can change 'inner' to 'outer', 'left', or 'right' based on your requirement\n",
    "\n",
    "    models_merged['request_parameter'] = np.where(\n",
    "        models_merged['labelnaam_requests'].isna(),\n",
    "        models_merged['modelname'],\n",
    "        models_merged['labelnaam_requests']\n",
    "    )\n",
    "    return models_merged\n",
    "\n",
    "def brand_tracker(brand): \n",
    "    file_target = f\"data/tracking/{brand}_completed.txt\"\n",
    "    if not os.path.exists(file_target): \n",
    "        f = open(file_target, 'w+')\n",
    "        f.close()\n",
    "    with open(file_target, 'r', encoding='utf8') as file:\n",
    "        finished_models = file.readlines()\n",
    "        finished_models = [model.strip() for model in finished_models]\n",
    "    return finished_models\n",
    "\n",
    "def completed_model_of_brand(brand, model):\n",
    "    file_target = f\"data/tracking/{brand}_completed.txt\"\n",
    "    with open(file_target, 'a', encoding='utf8') as file:\n",
    "        file.write(model + \"\\n\")\n",
    "        file.flush()\n",
    "\n",
    "def completed_brand(brand):\n",
    "    with open('data/tracking/brands_completed.txt', 'a', encoding='utf8') as file:\n",
    "        file.write(brand + \"\\n\")\n",
    "        file.flush()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab904b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a9be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_models('kia')\n",
    "with open('data/tracking/brands_completed.txt', 'r', encoding='utf8') as file:\n",
    "    finished_brands = file.readlines()\n",
    "    finished_brands = [brand.strip() for brand in finished_brands]\n",
    "\n",
    "def extract_counter(html_content):\n",
    "    c = html_content.content\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "    json_data = json.loads(json_string_data.string)\n",
    "    return json_data['props']['pageProps']['numberOfResults']\n",
    "\n",
    "      \n",
    "\n",
    "def validate_model_name(brand, model):\n",
    "    #check that the model name is parsed correctly!\n",
    "    #   if the model is not parsed okay, then the count result is the same!\n",
    "    all_for_brand_url = f\"https://www.autoscout24.be/nl/lst/{brand}\"\n",
    "    all_for_brand_model_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}\"\n",
    "    brand_repl = backoff_strategy(all_for_brand_url)\n",
    "    brand_model_repl = backoff_strategy(all_for_brand_model_url)\n",
    "    return extract_counter(brand_repl) != extract_counter(brand_model_repl)\n",
    "\n",
    "def log_model_error(brand, model):\n",
    "    with open('data/logging/error_brand_model.txt', 'a+') as file:\n",
    "        file.write(f\"{brand}, {model}, {normalize_name(model)}\\n\")\n",
    "        file.flush()\n",
    "\n",
    "# print(validate_model_name('volvo', 'c30'))\n",
    "# print(validate_model_name('volvo', 'clio'))\n",
    "# print(validate_model_name('volkswagen', 't61'))\n",
    "# print(validate_model_name('volkswagen', 't6.1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 62/76 [1:40:21<14:04, 60.29s/it]   "
     ]
    }
   ],
   "source": [
    "brands = ['alfa-romeo', 'fiat', 'mazda', 'seat', 'skoda', 'volvo', 'hyundai',\n",
    "           'lexus', 'lotus', 'porsche', 'audi', 'volkswagen', 'ford',\n",
    "             'mercedes-benz', 'nissan', 'renault', 'peugeot', 'opel', 'jeep', \n",
    "             'dacia', 'mini', 'land-rover', 'toyota', 'subaru','kia',  'suzuki', 'honda', 'citroen', 'alpine']\n",
    "#BMW is a bit of an annoying case!\n",
    "\n",
    "for brand in brands: \n",
    "    if brand in finished_brands:\n",
    "        continue\n",
    "    models_done = brand_tracker(brand)\n",
    "    models_merged = get_models(brand)\n",
    "    assert(models_merged.request_parameter.isna().sum() == 0)\n",
    "    for model in tqdm(models_merged.request_parameter.unique()):\n",
    "        storage = {}\n",
    "        if model.lower().strip() == 'others':\n",
    "            continue\n",
    "        if model in models_done:\n",
    "            continue\n",
    "        as_model = normalize_name(model)\n",
    "\n",
    "        if not validate_model_name(brand, as_model):\n",
    "            log_model_error(brand, model)\n",
    "            continue\n",
    "        \n",
    "        low, high = extract_dates(brand, model)\n",
    "        \n",
    "        if low == False and high == False:\n",
    "            continue\n",
    "        for year in range(low, high+1):\n",
    "            for page in range(1, 21):\n",
    "                url = f\"https://www.autoscout24.be/nl/lst/{brand}/{as_model}/re_{year}?atype=C&cy=B&damaged_listing=exclude&desc=0&page={page}\"\n",
    "                r = session.get(url)\n",
    "                r.status_code\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "                listings = soup.find_all('article')\n",
    "                if(len(listings) == 0): \n",
    "                    break\n",
    "\n",
    "                json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "                json_data = json.loads(json_string_data.string)\n",
    "                json_listings = json_data['props']['pageProps']['listings']\n",
    "                images_of_listings = {\n",
    "                    key['id']: ['/'.join(image.split('/')[:-1]) + '/750x564.webp' for image in key['images']]\n",
    "                    for key in json_listings\n",
    "                }\n",
    "                for listing in listings:\n",
    "                    first_registration = listing.get('data-first-registration')\n",
    "                    listing_id = listing.get('data-guid')\n",
    "                    make_name = listing.get('data-make')\n",
    "                    model_name = listing.get('data-model')\n",
    "                    price = listing.get('data-price')\n",
    "                    mileage = listing.get('data-mileage')\n",
    "                    if mileage.lower().strip() == 'unknown':\n",
    "                        mileage = -1\n",
    "                    #model taxonomy MAY be useful for linking chassis codes, so parse it: \n",
    "                    model_taxonomy = listing.get('data-model-taxonomy')\n",
    "                    cleaned_string = model_taxonomy.lstrip(\"[\").rstrip(\"];\")\n",
    "                    pairs = [pair.split(\":\") for pair in cleaned_string.split(\", \")]\n",
    "                    taxonomy_dict = {key.strip(): value.strip() for key, value in pairs}\n",
    "                    make_id = taxonomy_dict['make_id']\n",
    "                    model_id = taxonomy_dict['model_id']\n",
    "                    variant_id = taxonomy_dict['variant_id'] if taxonomy_dict['variant_id'] != '' else None\n",
    "                    generation_id = taxonomy_dict['generation_id'] if taxonomy_dict['generation_id'] != '' else None\n",
    "                    motortype_id = taxonomy_dict['motortype_id'] if taxonomy_dict['motortype_id'] != '' else None\n",
    "                    trim_id = taxonomy_dict['trim_id'] if taxonomy_dict['trim_id'] != '' else None\n",
    "                    # url_listing = listing.find('a', href=True)['href']\n",
    "                    storage_dir = os.path.join(basedir, brand, model, listing_id)\n",
    "                    images = images_of_listings[listing_id]\n",
    "                    shell, doors, weight, norm = get_listing_details(listing_id)\n",
    "                    if weight is not None:\n",
    "                        weight = weight.split(' ')[0]\n",
    "                    data_listing = [listing_id, brand, model, year, first_registration, make_name, model_name, price, mileage, make_id, model_id, variant_id, generation_id, motortype_id, trim_id, shell, doors, weight, norm]\n",
    "\n",
    "                    # insert_id = db.execute_query(query_listing, data_listing, True)\n",
    "                    images = download_images(images, storage_dir, 10, True)\n",
    "                    storage[listing_id] = {\n",
    "                        'listing_data': data_listing,\n",
    "                        'listing_images': images\n",
    "                    }\n",
    "\n",
    "        #Improved transaction handle with reduced database locking!\n",
    "        db.start_transaction()\n",
    "        query_image = \"\"\"INSERT INTO \n",
    "            automotive.images (listing_id, image_path)\n",
    "            VALUES(%s, %s)\"\"\"\n",
    "        query_listing = \"\"\"INSERT INTO \n",
    "            automotive.listings \n",
    "                (autoscout_id, brand, model, `year`, \n",
    "                first_registration, make_name_autoscout, \n",
    "                model_name_autoscout, price,mileage, \n",
    "                make_id, model_id, variant_id, generation_id,\n",
    "                motortype_id, trim_id, shelltype, doorcount,\n",
    "                weight, normlabel)\n",
    "                VALUES (%s, %s, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\"\"\n",
    "                   \n",
    "        for listing in storage:\n",
    "            metadata = storage[listing]['listing_data']\n",
    "            insert_id = 0\n",
    "            insert_id = db.execute_query(query_listing, metadata, True)\n",
    "            if insert_id == 0:\n",
    "                raise ValueError(\"Failed to retrieve listing ID.\")\n",
    " \n",
    "            image_data = storage[listing]['listing_images']\n",
    "            for image in image_data:\n",
    "                #remove the basedir from the path - this way we can move data to other computers\n",
    "                # as long as we point to relative folder.\n",
    "                image = image.replace(basedir, '').lstrip('\\\\')\n",
    "                data_image = [insert_id, image ]\n",
    "                db.execute_query(query_image, data_image, False)\n",
    "        db.commit_transaction()\n",
    "        #end of improved transaction handles!\n",
    "\n",
    "        completed_model_of_brand(brand, model)\n",
    "\n",
    "\n",
    "                \n",
    "    completed_brand(brand)\n",
    "db.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d03af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24c88fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43murl\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_name(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.rollback_transaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7d950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automotive_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
