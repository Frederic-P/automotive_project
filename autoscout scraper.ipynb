{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625eb8c4-9540-41d7-a948-e13b9f008238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "sys.path.append('utils')\n",
    "import os\n",
    "import config_handling as conf\n",
    "from multithread_image_ripper import download_images\n",
    "from database import Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab4a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "config = conf.read_config('automotive.conf.ini')\n",
    "config.read('config.ini')\n",
    "connection_type = config['settings']['connection']\n",
    "connection_type\n",
    "user = config[connection_type]['user']\n",
    "pw = config[connection_type]['pw']\n",
    "host = config[connection_type]['host']\n",
    "db = config[connection_type]['db']\n",
    "port = config[connection_type].getint('port')\n",
    "db = Database(host,\n",
    "              port,\n",
    "              user,\n",
    "              pw,\n",
    "              db\n",
    "              )\n",
    "db.connect()\n",
    "db.start_transaction()\n",
    "#image directory: \n",
    "basedir = config['settings']['image_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e4ab8-7e9a-4970-960d-b18a84c3c8a9",
   "metadata": {},
   "source": [
    "## 1. Brand extraction: \n",
    "Start by getting all the brand codes, these do not match the codes used by Autodoc in the previous step of the data scraping process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7b6cbc-58f7-4ebf-a4f7-d663996ffdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at = \"https://www.autoscout24.be/nl/\"\n",
    "session = requests.Session()\n",
    "r = session.get(start_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90348d56-506e-49b9-9008-81bd66fe43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(given_name): \n",
    "    # Lowers and removes all non-alphanumeric characters from a given input string,\n",
    "    # and replaces spaces with hyphens. Some cars have special symbols, we need to\n",
    "    # keep them for autoscout to work. \n",
    "    special = [' ', '-', '/', '(', ')', '.', '+']\n",
    "    alphanums = list([symbol for symbol in given_name if symbol.isalpha() or symbol.isnumeric() or symbol in special])\n",
    "    normalized_name = ''.join(alphanums).lower()\n",
    "    normalized_name = normalized_name.replace('/','%2f')\n",
    "    return normalized_name.replace(' ', '-')\n",
    "\n",
    "#startu): get all the brands \n",
    "r.content\n",
    "soup = BeautifulSoup(r.content)\n",
    "brands = soup.select(\"#make\")[0].find_all('option')\n",
    "l = {}\n",
    "for brand in brands:\n",
    "    value = brand['value']\n",
    "    name = normalize_name(brand.text)\n",
    "    if value != '': \n",
    "        l[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1013609-299a-4053-8175-d81300967a4f",
   "metadata": {},
   "source": [
    "## 2. Model extraction\n",
    "For model extraction we can use XHR requests again to a JSON endpoint (for this we need the numerical brand id from the previous extraction step). Model hierarchy is not the same for all brands.\n",
    "\n",
    "For instance the BMW 1-series is a valid model designation, autoscout has this as a parent element for 1-series cars with different engines in it (e.g. 114, 116, 120). In this case xyy can be interpreted as: x = series designation, yy = engine displacement /100 (114 has a 1400CC engine, 116 has a 1600CC engine...). Each of the different engine displacement is considered to be a seperate submodel to the 1 series by Autoscout - even if there's visually no difference between the cars.\n",
    "The problem is with X-series carsfor BMW, where X1, X3, X5 are distinct cars but are all grouped in the same 'family' of cars.\n",
    "\n",
    "For other brands there is no such hierarchy, e.g. Toyota where the response is a flat list of modelnames without differntiating different bodystyles (e.g. Corrola hatch or Corrola SW). Here the data will return both SW and hatch body styles for the same model. The visual difference between an SW and a hatch model is quite stark and easy to recognize for the human observer. \n",
    "\n",
    "In spite of these problems, it would be a good idea to have the scraper collect model by model - in stead of collecting all data for the entire brand. The advantage of this approach is that you have an identifiable target. (i.e. you know the car is of brand X and model Y), then you can use features on autoscout such as doorcount, inscription year and bodystyle (which all are subject to human errors); to correctly link it to chassis codes extracted from autodoc. \n",
    "\n",
    "One caveat with the inscription year is that this value is not the same as a production year!!! Generally speaking production starts well in advance of car sales and older stock models can still be registered while the new model is in production. This means there'll be an overlap - not much to do about it; we might consider dropping the first three months of sales for a given model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0629c9",
   "metadata": {},
   "source": [
    "Autoscout allows a max of 20 pages with 20 results each. So you have a hard limit of 400 listings for a given brand/model combination. We don't care much about pagination, as exceeding the first 400 results will automatically show the same message as 'all results have been viewed'. We can solve the issue by restricting registration years X to Y where the range X-Y 0 (iterating over all cars from a specific year)\n",
    "\n",
    "when dealing with images downloaded from Autoscout, you'll have the data as compressed files (.webp), YOLO does not support this and I don't know about other ML models, so the multithreaded ripper should have .jpg conversion built in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0294b996-90d8-46e4-a6d8-302a598ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_strategy(at, extract_json = False):\n",
    "    \"\"\"custom scraper that will try to collect data at a url for a max of 5 times\n",
    "        at = str = URL\n",
    "        extract_json = bool = whether to extract json from the pagesource or not\n",
    "    \"\"\"\n",
    "    success = False\n",
    "    x = 1\n",
    "    while not success and x <= 6: \n",
    "        r = session.get(at)\n",
    "        if r.status_code == 200:\n",
    "            if not extract_json:\n",
    "                success = True\n",
    "                return r\n",
    "            else:\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "                json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "                json_data = json.loads(json_string_data.string)\n",
    "                valid_parse = (\n",
    "                    'props' in json_data and\n",
    "                    'pageProps' in json_data['props'] and\n",
    "                    'listingDetails' in json_data['props']['pageProps'] and\n",
    "                    'vehicle' in json_data['props']['pageProps']['listingDetails']\n",
    "                )\n",
    "                if json_data is not None and valid_parse:\n",
    "                    success = True\n",
    "                    return json_data\n",
    "        time.sleep(x*10)\n",
    "        x+=1\n",
    "    return r\n",
    "\n",
    "def extract_dates(brand, model, country):\n",
    "    \"\"\"\n",
    "        extracts the earliest and latest dates of selling a car identified by brand and model\n",
    "        in a give country. Will return the years. \n",
    "        brand = str = brand of the car\n",
    "        model = str = model of the car\n",
    "        country = str = lettercode used by autoscout to identify a country\n",
    "    \"\"\"\n",
    "    earliest_year_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}?atype=C&cy={country}&damaged_listing=exclude&desc=0&sort=year\"\n",
    "    latest_year_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}?atype=C&cy={country}&damaged_listing=exclude&desc=1&sort=year\"\n",
    "    # r_start = session.get(earliest_year_url)\n",
    "    r_start = backoff_strategy(earliest_year_url)\n",
    "    soup_start = BeautifulSoup(r_start.content, 'html.parser')\n",
    "    first_articles = soup_start.find_all('article')\n",
    "    if len(first_articles) == 0:    #No cars for sale\n",
    "        return[False, False]\n",
    "    first_date = first_articles[0].get('data-first-registration')\n",
    "    # r_stop = session.get(latest_year_url)   #defintely cars for sale, no need to check.\n",
    "    r_stop = backoff_strategy(latest_year_url)\n",
    "    soup_stop = BeautifulSoup(r_stop.content, 'html.parser')\n",
    "    last_date = soup_stop.find_all('article')[0].get('data-first-registration')\n",
    "    if first_date.lower() == 'new':\n",
    "        first_year = 2024\n",
    "    else:\n",
    "        first_year = first_date.split('-')[1]\n",
    "    if last_date.lower() == 'new' or last_date.lower() == 'unknown':\n",
    "        last_year = 2024\n",
    "    else:\n",
    "        last_year = last_date.split('-')[1]\n",
    "    return [int(first_year), int(last_year)]\n",
    "\n",
    "def get_listing_details(listing):\n",
    "    \"\"\"doorcount and chassistype would be handy to have, it's unfortunately not part of the\n",
    "    JSON response so we need to do one request per listing to extract this usefull information.\n",
    "    There are other useful bits of information too that might help the disambiguation process.\n",
    "\n",
    "    The good thing is it's quite easy to parse as they made it accessible in JSON format\n",
    "    listing = str (UUIDV4) formatted string representing the primary key used by autoscout for the listing. \n",
    "    \"\"\"\n",
    "    url = f\"https://www.autoscout24.be/nl/aanbod/{listing}\"\n",
    "    json_data = backoff_strategy(url, True)\n",
    "    # soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    # json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "    # json_data = json.loads(json_string_data.string)\n",
    "    data = json_data['props']['pageProps']['listingDetails']['vehicle']\n",
    "    shell = data['bodyType']\n",
    "    doors = data['numberOfDoors']\n",
    "    weight = data['weight']\n",
    "    norm_data = data['environmentEuDirective']\n",
    "    if norm_data is not None:\n",
    "        norm = norm_data['label']\n",
    "    else:\n",
    "        norm = None\n",
    "    return [shell, doors, weight, norm]\n",
    "\n",
    "def get_models(brand): \n",
    "    headers = {\n",
    "    \"Accept\": \"application/json\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    #NOTE: API requests are rejected if you don't have a session cookie; that's why we use session.get and not the default request.get\n",
    "    url = f\"https://www.autoscout24.be/as24-home/api/taxonomy/cars/makes/{l[brand]}/models\"\n",
    "    r = session.get(url, headers= headers)\n",
    "    #IF the modelLineId is None, than the model_id is a code on itself. (Toyota casus)\n",
    "    # IF modelLineId is not None, then the model_id is to be considered a subcode of the modelLineId (BMW casus)\n",
    "    models = []\n",
    "    model_lines = []\n",
    "    for model in r.json()['models']['model']['values']:\n",
    "        model_name = model['name']\n",
    "        model_id = model['id']\n",
    "        models.append([model_name, model_id, model['modelLineId']])\n",
    "    for model_line in r.json()['models']['modelLine']['values']:\n",
    "        model_line_id = model_line['id']\n",
    "        model_line_name = model_line['name']\n",
    "        dutch_name = model_line['label']['nl_BE']\n",
    "        model_lines.append([model_line_id, model_line_name, dutch_name])\n",
    "    model_df = pd.DataFrame(models, columns=['modelname','model_id','parent_id'])\n",
    "    model_lines_df = pd.DataFrame(model_lines, columns=['line_id', 'modelline_name', 'labelnaam_requests'])\n",
    "\n",
    "    models_merged = pd.merge(model_df, model_lines_df, left_on='parent_id', right_on='line_id', how='outer')  # You can change 'inner' to 'outer', 'left', or 'right' based on your requirement\n",
    "\n",
    "    models_merged['request_parameter'] = np.where(\n",
    "        models_merged['labelnaam_requests'].isna(),\n",
    "        models_merged['modelname'],\n",
    "        models_merged['labelnaam_requests']\n",
    "    )\n",
    "    return models_merged\n",
    "\n",
    "def brand_tracker(country, brand): \n",
    "    countrydir = f\"data/tracking/{country}\"\n",
    "    if not os.path.exists(countrydir):\n",
    "        os.mkdir(countrydir)\n",
    "    file_target = f\"data/tracking/{country}/{brand}_completed.txt\"\n",
    "    if not os.path.exists(file_target): \n",
    "        f = open(file_target, 'w+')\n",
    "        f.close()\n",
    "    with open(file_target, 'r', encoding='utf8') as file:\n",
    "        finished_models = file.readlines()\n",
    "        finished_models = [model.strip() for model in finished_models]\n",
    "    return finished_models\n",
    "\n",
    "def completed_model_of_brand(country, brand, model):\n",
    "    file_target = f\"data/tracking/{country}/{brand}_completed.txt\"\n",
    "    with open(file_target, 'a', encoding='utf8') as file:\n",
    "        file.write(model + \"\\n\")\n",
    "        file.flush()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a9be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_counter(html_content):\n",
    "    c = html_content.content\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "    json_data = json.loads(json_string_data.string)\n",
    "    return json_data['props']['pageProps']['numberOfResults']\n",
    "\n",
    "      \n",
    "\n",
    "def validate_model_name(brand, model):\n",
    "    #check that the model name is parsed correctly!\n",
    "    #   if the model is not parsed okay, then the count result is the same OR HIGHER\n",
    "    #   if the model is parsed correctly and the count is ZERO then the model has no listings\n",
    "    all_for_brand_url = f\"https://www.autoscout24.be/nl/lst/{brand}\"\n",
    "    all_for_brand_model_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}\"\n",
    "    brand_repl = backoff_strategy(all_for_brand_url)\n",
    "    brand_model_repl = backoff_strategy(all_for_brand_model_url)\n",
    "    # print(extract_counter(brand_model_repl))\n",
    "    if extract_counter(brand_model_repl) == 0:\n",
    "        return True\n",
    "    return extract_counter(brand_repl) != extract_counter(brand_model_repl)\n",
    "\n",
    "\n",
    "def log_model_error(brand, model):\n",
    "    with open('data/logging/error_brand_model.txt', 'a+') as file:\n",
    "        file.write(f\"{brand}, {model}, {normalize_name(model)}\\n\")\n",
    "        file.flush()\n",
    "\n",
    "# print(validate_model_name('volvo', 'c30'))\n",
    "# print(validate_model_name('volvo', 'clio'))\n",
    "# print(validate_model_name('volkswagen', 't61'))\n",
    "# print(validate_model_name('volkswagen', 't6.1'))\n",
    "\n",
    "filedirs = {\n",
    "    'brand_tracking': 'data/tracking/'\n",
    "}\n",
    "\n",
    "def get_brandcompletion_for_country(countrycode): \n",
    "    #todo test\n",
    "    brandfile = os.path.join(filedirs['brand_tracking'], f\"brands_completed_{countrycode}.txt\")\n",
    "    if not os.path.exists(brandfile):\n",
    "        x = open(brandfile, 'a+')\n",
    "        x.close()\n",
    "        return []\n",
    "    else: \n",
    "        with open(brandfile, 'r', encoding='utf8') as file:\n",
    "            finished_brands = file.readlines()\n",
    "            finished_brands = [brand.strip().lower() for brand in finished_brands]\n",
    "            return finished_brands\n",
    "        \n",
    "def completed_brand_in_country(countrycode, brand):\n",
    "    brandfile = os.path.join(filedirs['brand_tracking'], f\"brands_completed_{countrycode}.txt\")\n",
    "    with open(brandfile, 'a+', encoding='utf8') as file:\n",
    "        file.write(brand + \"\\n\")\n",
    "        file.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7b5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count(*)': 1}]\n"
     ]
    }
   ],
   "source": [
    "def extract_all_listings(country, brand, model, as_model):\n",
    "    storage = {}   \n",
    "    #what's the daterange?: \n",
    "    low, high = extract_dates(brand, model, country)\n",
    "    # print(low, high)\n",
    "    if low == False and high == False:\n",
    "        return {}\n",
    "    #iterate over the daterange:\n",
    "    for year in range(low, high+1):\n",
    "        #max 20 pages \n",
    "        for page in range(1, 21):\n",
    "            url = f\"https://www.autoscout24.be/nl/lst/{brand}/{as_model}/re_{year}?atype=C&cy={country}&damaged_listing=exclude&desc=0&page={page}\"\n",
    "            r = session.get(url)\n",
    "            r.status_code\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            listings = soup.find_all('article')\n",
    "            if(len(listings) == 0): \n",
    "                break\n",
    "            json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "            json_data = json.loads(json_string_data.string)\n",
    "            json_listings = json_data['props']['pageProps']['listings']\n",
    "            images_of_listings = {\n",
    "                key['id']: ['/'.join(image.split('/')[:-1]) + '/750x564.webp' for image in key['images']]\n",
    "                for key in json_listings\n",
    "            }\n",
    "            for listing in listings:\n",
    "                first_registration = listing.get('data-first-registration')\n",
    "                listing_id = listing.get('data-guid')\n",
    "                make_name = listing.get('data-make')\n",
    "                model_name = listing.get('data-model')\n",
    "                price = listing.get('data-price')\n",
    "                mileage = listing.get('data-mileage')\n",
    "                if mileage.lower().strip() == 'unknown':\n",
    "                    mileage = -1\n",
    "                #model taxonomy MAY be useful for linking chassis codes, so parse it: \n",
    "                model_taxonomy = listing.get('data-model-taxonomy')\n",
    "                cleaned_string = model_taxonomy.lstrip(\"[\").rstrip(\"];\")\n",
    "                pairs = [pair.split(\":\") for pair in cleaned_string.split(\", \")]\n",
    "                taxonomy_dict = {key.strip(): value.strip() for key, value in pairs}\n",
    "                #print(taxonomy_dict)\n",
    "                make_id = taxonomy_dict['make_id']\n",
    "                #model taxonomy is very messy on autoscout: model_id, modelId is used too for some Mercedes cars. No documentation on this.\n",
    "                #preference of scraping model_id > modelId > model_group_id >modelGroupIds (carefull that's a list!!!), defaulto -1\n",
    "                model_id = taxonomy_dict.get(\n",
    "                    'model_id', \n",
    "                    taxonomy_dict.get('modelId',\n",
    "                    taxonomy_dict.get('model_group_id',\n",
    "                    taxonomy_dict.get('modelGroupIds', -1)))\n",
    "                    )\n",
    "                ##print(taxonomy_dict)\n",
    "                #print(model_id)\n",
    "                if isinstance(model_id, list):\n",
    "                    model_id = model_id[0]\n",
    "                if model_id == '':\n",
    "                    model_id = None\n",
    "                #print('corrected to ', model_id)\n",
    "                variant_id = taxonomy_dict['variant_id'] if taxonomy_dict['variant_id'] != '' else None\n",
    "                generation_id = taxonomy_dict['generation_id'] if taxonomy_dict['generation_id'] != '' else None\n",
    "                motortype_id = taxonomy_dict['motortype_id'] if taxonomy_dict['motortype_id'] != '' else None\n",
    "                trim_id = taxonomy_dict['trim_id'] if taxonomy_dict['trim_id'] != '' else None\n",
    "                # url_listing = listing.find('a', href=True)['href']\n",
    "                storage_dir = os.path.join(basedir, brand, model, listing_id)\n",
    "                images = images_of_listings[listing_id]\n",
    "                shell, doors, weight, norm = get_listing_details(listing_id)\n",
    "                if weight is not None:\n",
    "                    weight = weight.split(' ')[0]\n",
    "                data_listing = [listing_id, brand, model, year, first_registration, make_name, model_name, price, mileage, make_id, model_id, variant_id, generation_id, motortype_id, trim_id, shell, doors, weight, norm, country]\n",
    "\n",
    "                # insert_id = db.execute_query(query_listing, data_listing, True)\n",
    "                images = download_images(images, storage_dir, 10, True)\n",
    "                storage[listing_id] = {\n",
    "                    'listing_data': data_listing,\n",
    "                    'listing_images': images\n",
    "                }\n",
    "    return storage\n",
    "\n",
    "def check_listing_presence(id):\n",
    "    \"\"\"\n",
    "        Part of the maintainer logic: checks the given id (uuidv4) against the listings table.\n",
    "        If the id is in the listings table, the record has been sraped already and can be skipped.\n",
    "        returns: Bool (true/false)\n",
    "    \"\"\"\n",
    "    query = \"SELECT count(*) FROM listings WHERE listings.autoscout_id = %s\" \n",
    "    data = [id]\n",
    "    result = db.execute_query(query, data)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "check_listing_presence('2f32b42f-ed86-4859-9666-605f181775e3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56ddf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_into_sql(storage): \n",
    "    \"\"\"Saves all information on one model in the listings and images tables.\"\"\"\n",
    "    #Improved transaction handle with reduced database locking!\n",
    "    db.start_transaction()\n",
    "    query_image = \"\"\"INSERT INTO \n",
    "        automotive.images (listing_id, image_path)\n",
    "        VALUES(%s, %s)\"\"\"\n",
    "    query_listing = \"\"\"INSERT INTO \n",
    "        automotive.listings \n",
    "            (autoscout_id, brand, model, `year`, \n",
    "            first_registration, make_name_autoscout, \n",
    "            model_name_autoscout, price,mileage, \n",
    "            make_id, model_id, variant_id, generation_id,\n",
    "            motortype_id, trim_id, shelltype, doorcount,\n",
    "            weight, normlabel, countrycode)\n",
    "            VALUES (%s, %s, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\"\"\n",
    "            \n",
    "    for listing in storage:\n",
    "        metadata = storage[listing]['listing_data']\n",
    "        insert_id = 0\n",
    "        insert_id = db.execute_query(query_listing, metadata, True)\n",
    "        if insert_id == 0:\n",
    "            raise ValueError(\"Failed to retrieve listing ID.\")\n",
    "\n",
    "        image_data = storage[listing]['listing_images']\n",
    "        for image in image_data:\n",
    "            #remove the basedir from the path - this way we can move data to other computers\n",
    "            # as long as we point to relative folder.\n",
    "            image = image.replace(basedir, '').lstrip('\\\\')\n",
    "            data_image = [insert_id, image ]\n",
    "            db.execute_query(query_image, data_image, False)\n",
    "    db.commit_transaction()\n",
    "    #end of improved transaction handles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 67/83 [10:59<03:32, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process https://prod.pictures.autoscout24.net/listing-images/eb6fbb20-3966-40b0-a9fa-ec047b53a4cc_cc2054e5-5c1d-42cc-a903-0262c01673f8.jpg/750x564.webp: HTTPSConnectionPool(host='prod.pictures.autoscout24.net', port=443): Max retries exceeded with url: /listing-images/eb6fbb20-3966-40b0-a9fa-ec047b53a4cc_cc2054e5-5c1d-42cc-a903-0262c01673f8.jpg/750x564.webp (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:992)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 82/83 [1:12:48<00:54, 54.18s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process https://prod.pictures.autoscout24.net/listing-images/6f93b36e-a94b-430e-8a7c-e541a3b628d7_c8757730-7c07-4dd1-b53e-843abb3560bb.jpg/750x564.webp: HTTPSConnectionPool(host='prod.pictures.autoscout24.net', port=443): Max retries exceeded with url: /listing-images/6f93b36e-a94b-430e-8a7c-e541a3b628d7_c8757730-7c07-4dd1-b53e-843abb3560bb.jpg/750x564.webp (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:992)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [1:16:03<00:00, 54.99s/it]\n",
      "100%|██████████| 65/65 [7:35:27<00:00, 420.42s/it]    \n",
      " 56%|█████▌    | 29/52 [53:05<1:50:01, 287.04s/it]"
     ]
    }
   ],
   "source": [
    "brands = ['alfa-romeo', 'fiat', 'mazda', 'seat', 'skoda', 'volvo', 'hyundai',\n",
    "           'lexus', 'lotus', 'porsche', 'audi', 'volkswagen', 'ford',\n",
    "             'mercedes-benz', 'nissan', 'renault', 'peugeot', 'opel', 'jeep', \n",
    "             'dacia', 'mini', 'land-rover', 'toyota', 'subaru','kia',  'suzuki',\n",
    "             'honda', 'citroen', 'alpine', 'bmw']\n",
    "countries = ['D', 'F', 'NL', 'B']\n",
    "# todo: You need to add BMW for Belgium\n",
    "# test code\n",
    "# implement maintainer\n",
    "# implement fix of error logged brand\n",
    "# \n",
    "for country in countries: \n",
    "    for brand in brands: \n",
    "        finished_brands = get_brandcompletion_for_country(country)\n",
    "        if brand in finished_brands:\n",
    "            continue\n",
    "        models_done = brand_tracker(country, brand)\n",
    "        models_merged = get_models(brand)\n",
    "        assert(models_merged.request_parameter.isna().sum() == 0)\n",
    "        for model in tqdm(models_merged.request_parameter.unique()):\n",
    "            storage = {}\n",
    "            if model.lower().strip() == 'others':\n",
    "                continue\n",
    "            if model in models_done:\n",
    "                continue\n",
    "            as_model = normalize_name(model)\n",
    "\n",
    "            if not validate_model_name(brand, as_model):\n",
    "                log_model_error(brand, model)\n",
    "                continue\n",
    "            storage = extract_all_listings(country, brand, model, as_model)\n",
    "            save_data_into_sql(storage)\n",
    "\n",
    "\n",
    "\n",
    "            completed_model_of_brand(country, brand, model)\n",
    "\n",
    "\n",
    "                    \n",
    "        completed_brand_in_country(country, brand)\n",
    "db.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c524df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automotive_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
