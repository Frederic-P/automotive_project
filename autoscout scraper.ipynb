{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625eb8c4-9540-41d7-a948-e13b9f008238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('utils')\n",
    "import os\n",
    "import config_handling as conf\n",
    "from multithread_image_ripper import download_images\n",
    "from database import Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4a5a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configparser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Connect to database\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mconfigparser\u001b[49m\u001b[38;5;241m.\u001b[39mConfigParser()\n\u001b[0;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.ini\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m connection_type \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnection\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'configparser' is not defined"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "config = conf.read_config('automotive.conf.ini')\n",
    "config.read('config.ini')\n",
    "connection_type = config['settings']['connection']\n",
    "connection_type\n",
    "user = config[connection_type]['user']\n",
    "pw = config[connection_type]['pw']\n",
    "host = config[connection_type]['host']\n",
    "db = config[connection_type]['db']\n",
    "port = config[connection_type].getint('port')\n",
    "db = Database(host,\n",
    "              port,\n",
    "              user,\n",
    "              pw,\n",
    "              db\n",
    "              )\n",
    "db.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e4ab8-7e9a-4970-960d-b18a84c3c8a9",
   "metadata": {},
   "source": [
    "## 1. Brand extraction: \n",
    "Start by getting all the brand codes, these do not match the codes used by Autodoc in the previous step of the data scraping process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b6cbc-58f7-4ebf-a4f7-d663996ffdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at = \"https://www.autoscout24.be/nl/\"\n",
    "basedir = 'C:\\imdir'\n",
    "session = requests.Session()\n",
    "r = session.get(start_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90348d56-506e-49b9-9008-81bd66fe43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(given_name): \n",
    "    #lowers and removes all non alfanumerical characters from a given input string.\n",
    "    alphanums = list([symbol for symbol in given_name if symbol.isalpha() or symbol.isnumeric()])\n",
    "    return ''.join(alphanums).lower()\n",
    "\n",
    "r.content\n",
    "soup = BeautifulSoup(r.content)\n",
    "brands = soup.select(\"#make\")[0].find_all('option')\n",
    "l = {}\n",
    "for brand in brands:\n",
    "    value = brand['value']\n",
    "    name = normalize_name(brand.text)\n",
    "    if value != '': \n",
    "        l[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1013609-299a-4053-8175-d81300967a4f",
   "metadata": {},
   "source": [
    "## 2. Model extraction\n",
    "For model extraction we can use XHR requests again to a JSON endpoint (for this we need the numerical brand id from the previous extraction step). Model hierarchy is not the same for all brands.\n",
    "\n",
    "For instance the BMW 1-series is a valid model designation, autoscout has this as a parent element for 1-series cars with different engines in it (e.g. 114, 116, 120). In this case xyy can be interpreted as: x = series designation, yy = engine displacement /100 (114 has a 1400CC engine, 116 has a 1600CC engine...). Each of the different engine displacement is considered to be a seperate submodel to the 1 series by Autoscout - even if there's visually no difference between the cars.\n",
    "The problem is with X-series carsfor BMW, where X1, X3, X5 are distinct cars but are all grouped in the same 'family' of cars.\n",
    "\n",
    "For other brands there is no such hierarchy, e.g. Toyota where the response is a flat list of modelnames without differntiating different bodystyles (e.g. Corrola hatch or Corrola SW). Here the data will return both SW and hatch body styles for the same model. The visual difference between an SW and a hatch model is quite stark and easy to recognize for the human observer. \n",
    "\n",
    "In spite of these problems, it would be a good idea to have the scraper collect model by model - in stead of collecting all data for the entire brand. The advantage of this approach is that you have an identifiable target. (i.e. you know the car is of brand X and model Y), then you can use features on autoscout such as doorcount, inscription year and bodystyle (which all are subject to human errors); to correctly link it to chassis codes extracted from autodoc. \n",
    "\n",
    "One caveat with the inscription year is that this value is not the same as a production year!!! Generally speaking production starts well in advance of car sales and older stock models can still be registered while the new model is in production. This means there'll be an overlap - not much to do about it; we might consider dropping the first three months of sales for a given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd378b-19a6-42f4-8e41-5188aa55be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = 'alfaromeo'\n",
    "print(l[brand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"application/json\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "\n",
    "}\n",
    "#NOTE: API requests are rejected if you don't have a session cookie; that's why we use session.get and not the default request.get\n",
    "url = f\"https://www.autoscout24.be/as24-home/api/taxonomy/cars/makes/{l[brand]}/models\"\n",
    "r = session.get(url, headers= headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5a7ee-8652-4a1e-8e55-efe167fd184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF the modelLineId is None, than the model_id is a code on itself. (Toyota casus)\n",
    "# IF modelLineId is not None, then the model_id is to be considered a subcode of the modelLineId (BMW casus)\n",
    "models = []\n",
    "model_lines = []\n",
    "for model in r.json()['models']['model']['values']:\n",
    "    model_name = model['name']\n",
    "    model_id = model['id']\n",
    "    models.append([model_name, model_id, model['modelLineId']])\n",
    "for model_line in r.json()['models']['modelLine']['values']:\n",
    "    model_line_id = model_line['id']\n",
    "    model_line_name = model_line['name']\n",
    "    dutch_name = model_line['label']['nl_BE']\n",
    "    model_lines.append([model_line_id, model_line_name, dutch_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(models, columns=['modelname','model_id','parent_id'])\n",
    "model_lines_df = pd.DataFrame(model_lines, columns=['line_id', 'modelline_name', 'labelnaam_requests'])\n",
    "\n",
    "models_merged = pd.merge(model_df, model_lines_df, left_on='parent_id', right_on='line_id', how='outer')  # You can change 'inner' to 'outer', 'left', or 'right' based on your requirement\n",
    "\n",
    "models_merged['request_parameter'] = np.where(\n",
    "    models_merged['labelnaam_requests'].isna(),\n",
    "    models_merged['modelname'],\n",
    "    models_merged['labelnaam_requests']\n",
    ")\n",
    "models_merged\n",
    "assert(models_merged.request_parameter.isna().sum() == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ff7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0629c9",
   "metadata": {},
   "source": [
    "Autoscout allows a max of 20 pages with 20 results each. So you have a hard limit of 400 listings for a given brand/model combination. We don't care much about pagination, as exceeding the first 400 results will automatically show the same message as 'all results have been viewed'. We can solve the issue by restricting registration years X to Y where the range X-Y 0 (iterating over all cars from a specific year)\n",
    "\n",
    "when dealing with images downloaded from Autoscout, you'll have the data as compressed files (.webp), YOLO does not support this and I don't know about other ML models, so the multithreaded ripper should have .jpg conversion built in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294b996-90d8-46e4-a6d8-302a598ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping listings per brand/model: \n",
    "def extract_dates(brand, model):\n",
    "    earliest_year_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}?atype=C&cy=B&damaged_listing=exclude&desc=0&sort=year\"\n",
    "    latest_year_url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}?atype=C&cy=B&damaged_listing=exclude&desc=1&sort=year\"\n",
    "    r_start = session.get(earliest_year_url)\n",
    "    soup_start = BeautifulSoup(r_start.content, 'html.parser')\n",
    "    first_articles = soup_start.find_all('article')\n",
    "    if len(first_articles) == 0:    #No cars for sale\n",
    "        return[False, False]\n",
    "    first_date = first_articles[0].get('data-first-registration')\n",
    "    r_stop = session.get(latest_year_url)   #defintely cars for sale, no need to check.\n",
    "    soup_stop = BeautifulSoup(r_stop.content, 'html.parser')\n",
    "    last_date = soup_stop.find_all('article')[0].get('data-first-registration')\n",
    "    first_year = first_date.split('-')[1]\n",
    "    if last_date == 'new':\n",
    "        last_year = 2024\n",
    "    else:\n",
    "        last_year = last_date.split('-')[1]\n",
    "    return [int(first_year), int(last_year)]\n",
    "\n",
    "def get_listing_details(listing):\n",
    "    \"\"\"doorcount and chassistype would be handy to have, it's unfortunately not part of the\n",
    "    JSON response so we need to do one request per listing to extract this usefull information.\n",
    "    There are other useful bits of information too that might help the disambiguation process.\n",
    "\n",
    "    The good thing is it's quite easy to parse as they made it accessible in JSON format\n",
    "    \"\"\"\n",
    "    url = f\"https://www.autoscout24.be/nl/aanbod/{listing}\"\n",
    "    r = session.get(url)\n",
    "    r.status_code\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "    json_data = json.loads(json_string_data.string)\n",
    "    data = json_data['props']['pageProps']['listingDetails']['vehicle']\n",
    "    shell = data['bodyType']\n",
    "    doors = data['numberOfDoors']\n",
    "    weight = data['weight']\n",
    "    norm_data = data['environmentEuDirective']\n",
    "    if norm_data is not None:\n",
    "        norm = norm_data['label']\n",
    "    else:\n",
    "        norm = None\n",
    "    return [shell, doors, weight, norm]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model in models_merged.request_parameter.unique():\n",
    "    low, high = extract_dates(brand, model)\n",
    "    #print(brand, model, low, high) \n",
    "    \n",
    "    if low == False and high == False:\n",
    "        continue\n",
    "    for year in range(low, high+1):\n",
    "        for page in range(1, 21):\n",
    "            url = f\"https://www.autoscout24.be/nl/lst/{brand}/{model}/re_{year}?atype=C&cy=B&damaged_listing=exclude&desc=0&page={page}\"\n",
    "            r = session.get(url)\n",
    "            r.status_code\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            listings = soup.find_all('article')\n",
    "            #print(model, low, high, year, page, len(listings))\n",
    "            if(len(listings) == 0): \n",
    "                break\n",
    "\n",
    "            json_string_data = soup.find('script', id=\"__NEXT_DATA__\", type=\"application/json\")\n",
    "            json_data = json.loads(json_string_data.string)\n",
    "            json_listings = json_data['props']['pageProps']['listings']\n",
    "            images_of_listings = {\n",
    "                key['id']: ['/'.join(image.split('/')[:-1]) + '/750x564.webp' for image in key['images']]\n",
    "                for key in json_listings\n",
    "            }\n",
    "            for listing in listings:\n",
    "                first_registration = listing.get('data-first-registration')\n",
    "                listing_id = listing.get('data-guid')\n",
    "                make_name = listing.get('data-make')\n",
    "                model_name = listing.get('data-model')\n",
    "                price = listing.get('data-price')\n",
    "                mileage = listing.get('data-mileage')\n",
    "                #model taxonomy MAY be useful for linking chassis codes, so parse it: \n",
    "                model_taxonomy = listing.get('data-model-taxonomy')\n",
    "                cleaned_string = model_taxonomy.lstrip(\"[\").rstrip(\"];\")\n",
    "                pairs = [pair.split(\":\") for pair in cleaned_string.split(\", \")]\n",
    "                taxonomy_dict = {key.strip(): value.strip() for key, value in pairs}\n",
    "                make_id = taxonomy_dict['make_id']\n",
    "                model_id = taxonomy_dict['model_id']\n",
    "                variant_id = taxonomy_dict['variant_id']\n",
    "                generation_id = taxonomy_dict['generation_id']\n",
    "                motortype_id = taxonomy_dict['motortype_id']\n",
    "                trim_id = taxonomy_dict['trim_id']\n",
    "                # url_listing = listing.find('a', href=True)['href']\n",
    "                storage_dir = os.path.join(basedir, brand, model, listing_id)\n",
    "                images = images_of_listings[listing_id]\n",
    "                shell, doors, weight, norm = get_listing_details(listing_id)\n",
    "                print(brand, model, year, first_registration, listing_id, make_name, model_name, price, mileage, make_id, model_id, variant_id, generation_id, motortype_id, trim_id, shell, doors, weight, norm)\n",
    "                \n",
    "                download_images(images, storage_dir, 10, True)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be21dee-d2b5-4bc0-a093-e3e7df7059ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basedir, brand, model, listing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b886c-d1c5-4b5f-9238-363488f5a88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99e73d-44ec-4c06-8ddd-76de1ba4e227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3afc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31820e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05180c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automotive_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
